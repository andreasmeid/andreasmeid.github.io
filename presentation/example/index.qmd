---
format: 
  revealjs:
    theme: ["theme/q-theme.scss"]
    slide-number: c/t
    logo: "https://www.rstudio.com/wp-content/uploads/2018/10/RStudio-Logo-Flat.png"
    footer: "[andreasmeid.github.io/presentation/example](https://andreasmeid.github.io/presentation/example)"
    code-copy: true
    center-title-slide: false
    include-in-header: heading-meta.html
    code-link: true
    code-overflow: wrap
    highlight-style: a11y
    height: 1080
    width: 1920
execute: 
  eval: true
  echo: true
---

<h1> Outrageously efficient<br>exploratory data analysis </h1>

<h2> with Apache Arrow and `dplyr` </h2>

<hr>

<h3> Tom Mock, Customer Enablement Lead at </h3>

<h3> 2022-06-03 </h3>
<br>

<h3> `r fontawesome::fa("github", "black")` &nbsp; [github.com/jthomasmock/arrow-dplyr](https://github.com/jthomasmock/arrow-dplyr)

![](https://arrow.apache.org/img/offbrand_hex_2.png){.absolute top=425 left=1100 width="300"}
![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/dplyr.png){.absolute top=680 left=1250 width="300"}

![](https://www.rstudio.com/wp-content/uploads/2018/10/RStudio-Logo-Flat.png){.absolute top=360 left=965 height="65"}

## `arrow`

> `arrow` is software development platform for building high performance applications that process and transport large data sets  

. . .

* The `arrow` R package is an interface to data via the `arrow` backend, and has deep integration with `dplyr`:  
  - Ungrouped `mutate()`, `filter()`, `select()` was available in `arrow` 5.0  
  - `group_by()` + `summarize()` aggregation was added in `arrow` 6.0  
  - More complex `dplyr` operations were added in `arrow`7.0 and 8.0
  
. . .

* `arrow` data can also be "handed off" to `duckdb` with `to_duckdb()` for any `dbplyr` commands without data conversion. IE no serialization or data copying costs are incurred. 

## Working with bigger data?

* Relational databases (IE SQL) are still around and hugely popular but...  

. . .

* Data and specifically _local_ files are getting bigger  

. . .

* Additionally, many Data Warehouses/Data Lakes use flat-file storage (`.csv`, `.parquet`, `.json` etc) - there are query engines in many environments, but you can often end up with large extracts.

. . .

So, how do you work with data extracts that aren't already in a database, and are bigger than your memory?

## Pause for one second

If it _can_ fit in memory, then try out:  

* [`vroom::vroom()`](https://vroom.r-lib.org/) or [`data.table::fread()`](https://rdatatable.gitlab.io/data.table/reference/fread.html) for fast file reads _into_ R  
  * [`vroom(col_select = c(column_name))`](https://vroom.r-lib.org/reference/vroom.html) also allows for partial reads (ie specific columns)  
  * `arrow` itself also has crazy fast file reads on many file types

. . .

* [`data.table`](https://rdatatable.gitlab.io/data.table/index.html) or the `dplyr` front-end to `data.table` via [`dtplyr`](https://dtplyr.tidyverse.org/) for fast and efficient in-memory analysis  

. . .

* Lastly, the [`collapse`](https://sebkrantz.github.io/collapse/) R package for limited capability, but hyper-performant data manipulation  

## Fill your quiver with `arrow`s

```{r}
library(arrow)  # interface to arrow
library(dplyr)  # expressive and consistent interface for data analysis
library(tictoc) # timing out computations
```


[Using `tictoc` to [watch cute dog videos]{.fragment .strike fragment-index=2}]{.fragment fragment-index=1} [time our computations.]{.fragment fragment-index=2}


:::: {.columns .fragment fragment-index=1}

::: {.column width="50%"}

eins

:::

::: {.column width="50%"}
zwei
:::

::::


## `nflfastR` data

The data we're focused on today is big enough (1 million rows by 372 columns, about 2.2 GB uncompressed) and corresponds to _every_ NFL play in _every_ game from 1999 to 2021. The data is all available from the [{nflverse} Github](https://github.com/nflverse/nflfastR-data).

. . .

To use it efficiently, we've partitioned the data up into each season

:::: {.columns}

::: {.column width="50%"}

```
data-parquet/
├── 1999
│   └── data.parquet
├── 2000
│   └── data.parquet
├── 2001
│   └── data.parquet
├── 2002
│   └── data.parquet
├── 2003
│   └── data.parquet
├── 2004
│   └── data.parquet
├── 2005
│   └── data.parquet
├── 2006
│   └── data.parquet
├── 2007
│   └── data.parquet
├── 2008
│   └── data.parquet
├── 2009
│   └── data.parquet
├── 2010
│   └── data.parquet
├── 2011
│   └── data.parquet
├── 2012
│   └── data.parquet
├── 2013
│   └── data.parquet
├── 2014
│   └── data.parquet
```

:::

::: {.column width="50%"}

```

├── 2015
│   └── data.parquet
├── 2016
│   └── data.parquet
├── 2017
│   └── data.parquet
├── 2018
│   └── data.parquet
├── 2019
│   └── data.parquet
├── 2020
│   └── data.parquet
└── 2021
    └── data.parquet
```

:::

::::

## Nock the `arrow`

We can prepare our data to be used with `arrow::open_dataset()`

```{r}
ds <- open_dataset("data-parquet", partitioning = "season")

ds
```

## Pull the `arrow` back with `dplyr`

```{r}
#| code-line-numbers: "|8"
summarize_df <- ds |> 
  filter(season == 2021, play_type %in% c("pass", "run")) |> 
  filter(!is.na(epa)) |> 
  select(posteam, epa) |> 
  group_by(posteam) |> 
  summarize(n = n(), avg_epa = mean(epa))

print(summarize_df)
```

. . .

Note that while the computation has occurred, we really haven't "seen it" yet. Printing just reports back the 3x columns and their type.



## {background-image="howard.jpg" background-size="contain"}